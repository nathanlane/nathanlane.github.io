---
layout: post
title: 'Tutorial: A Beginner''s Guide to Scraping Historic Table Data'
date: 2014-10-05 02:05:28.000000000 +08:00
type: post
published: true
status: publish
categories:
- OCR for Economists
- Tutorials
tags:
- finereader
- ocr
- python
- regex
meta:
  pe_theme_meta: O:8:"stdClass":2:{s:7:"gallery";O:8:"stdClass":3:{s:2:"id";s:2:"89";s:5:"width";s:0:"";s:6:"height";s:0:"";}s:5:"video";O:8:"stdClass":1:{s:2:"id";s:3:"262";}}
  _edit_last: '1'
author:
 "Nathaniel"
---
this is crucial for recognizing table structure. In addition, ScanTailor also comes with a command line version that be used to script larger tasks.</p>
<p>&nbsp;</p>
<p>[caption id="" align="alignnone" width="923"]<div class="media image"><img src="{{ site.baseurl }}/assets/scantailor.png" alt="" width="923" height="501" /></div> Batch pre-processing images in ScanTailor's GUI-based version.[/caption]</p>
<h4>Other open source and programmatic tools for pre-processing.</h4>
<p>While most people use ImageMagick for basic image conversion, many people utilize its powerful features for <strong><a href="http://www.fmwconcepts.com/imagemagick/textcleaner/index.php" target="_blank">batch document cleaning scripts</a></strong>. <strong><a href="www.gimp.org/" target="_blank">GIMP</a></strong>, the popular open source graphic suite, also has promising batch pre-processing capabilities: people have had success with <strong><a href="http://gimper.net/resources/nuvola-tools.582/" target="_blank">Nuvola tools for cleaning up greyscale scans</a></strong>.</p>
<h3>3. OCRing with ABBYY FineReader 12.</h3>
<p>Now that we have a pile of cleaned TIFFs, we load the files into ABBYY FineReader for further  1) pre-processing, 2) training, 3) table analysis/OCRing, and 4) error verification:</p>
<h4>First, use ABBYY's pre-processing tools to further clean and select the optimal OCR resolution.</h4>
<p>Once loaded into FineReader, you will likely want to further clean the scans using the built-in pre-processing tools (<strong>"Edit Image"</strong>). One tool that is particular useful is the FineReader's optimal resolution tool, which scales the resolution of the image to maximize recognition.</p>
<h4>But before you OCR, train.</h4>
<p>Training is the next crucial step. With historic data, you will likely get poor results if you neglect to train the OCR software and jump straight into OCRing. The ability to fully train OCR engines distinguishes professional software from less sophisticated OCRing tools.</p>
<p>In general, training improves the ability of OCR algorithms to correctly classify characters by "tuning" the algorithm on your document's typeface. In FineReader, training is a trivial task, where you walk the program through recognizing a sample set of characters from your document. You can easily append and save these training files, called "<strong>User Patterns</strong>."</p>
<p>&nbsp;</p>
<p>[caption id="" align="alignnone" width="776"]<div class="media image"><img src="{{ site.baseurl }}/assets/abbytraining.png" alt="" width="776" height="546" /></div> Training ABBYY FineReader's OCR engine on a sample document.[/caption]</p>
<h4>"Analyzing" &amp; "Reading" - Table recognition &amp; OCRing in FineReader.</h4>
<p>In FineReader, layout recognition and OCR are known as "analyzing" and "reading", respectively. Unlike straight forward digitization of textual material, we want to make sure FineReader recognizes our table layouts and correct mishaps before it reads the content of individual table cells: First, the<strong> Analyze Selected Pages</strong> command detects the content of our pages (i.e. finds our tables). We then confirm that FineReader has recognized tables and table cells correctly, adjusting mistakes "by hand" with the built-in table editing tools. Second, we OCR the table contents with the <strong>Read Selected Pages</strong> command.</p>
<p>&nbsp;</p>
<p>[caption id="" align="alignnone" width="873"]<div class="media image"><img src="{{ site.baseurl }}/assets/abbyytable.png" alt="" width="873" height="519" /></div> A properly recognized table in ABBYY FineReader with OCRd content.[/caption]</p>
<h4>Check for mistakes, tweak, &amp; repeat.</h4>
<p>OCRing is never perfect. Once FineReader has "read" your document, you will want to check for errors. For each page, FineReader gives  a rough error rate, reporting the number of characters it is uncertain about on each document page.  The "Verify Text" tool allows us to easily check and correct each uncertain character.</p>
<p>In general, after the first OCRing it is best to get a sense of how successful character recognition was and the types of errors that occur. Often, exploring pre-processing and improved training can improve text recognition.</p>
<h3>5. Post-processing.</h3>
<p><span style="color: #666666;">Ultimately, FineReader will spit out .csv or .xlsx files, but newly digitized content still needs to be tidied up. </span></p>
<p><span style="color: #666666;">Especially if you’re working with old documents, OCRing produces some junk output; dust, scratches, and page discoloration can get picked up as weird symbols: *, ^, \, etc.. You can easily correct these blemishes </span><a style="color: #7a7a7a;" href="http://en.wikipedia.org/wiki/Regular_expression" target="_blank"><span style="font-weight: bold;">using regular expressions</span></a><span style="color: #666666;"> in your preferred scripting language (using </span><span style="font-weight: bold; color: #666666;">sub/gsub</span><span style="color: #666666;"> commands in R,</span><span style="font-weight: bold; color: #666666;"> re.sub</span><span style="color: #666666;"> type commands in Python). Better yet,</span><span style="font-weight: bold; color: #666666;"> <a style="color: #7a7a7a;" href="http://http//openrefine.org/" target="_blank">OpenRefine</a> </span><span style="color: #666666;">provides some extremely flexible tools for wrangling OCRd output, making most cleaning tasks trivial while also supporting advanced regular expression use.</span></p>
<p>[caption id="" align="alignnone" width="519"]<a href="http://openrefine.org/"><div class="media image"><img src="{{ site.baseurl }}/assets/openrefinelogo.png" alt="" width="519" height="125" /></div></a> Clean up weird OCR output using OpenRefine and regular expressions.[/caption]</p>
<p>&nbsp;</p>
<h3>Ending Note. A bit more on OCR software and getting advanced.</h3>
<p>Why ABBYY FineReader? First, the learning curve is lower than other open source options. The closest open source competition comes from <a href="https://code.google.com/p/tesseract-ocr/"><strong>Google’s tesseract OCR program</strong></a>, which is powerful and useful for those comfortable with the command line or OCRing from a preferred programming language. For advanced projects, tesseract offers unmatched flexibility and customization. However in my experience, the tesseract is frustrating to train and has poor table recognition ability.</p>
<p>Although it is relatively easy, ABBYY FineReader has downsides. For instance, the Mac version isn't as functional as the complete "Professional" PC version. Moreover, multi-core support is limited for both Professional and basic Corporate versions, making large projects slow and unwieldy (in my experience).</p>
<p>While FineReader provides tools for table area recognition, other times we have to pursue more programmatic methods of extracting table. Common approaches can be seen in <a href="http://www.propublica.org/nerds/item/image-to-text-ocr-and-imagemagick" target="_blank"><strong>Dan Nugyen’s ProPublica guide</strong></a> and <strong><a href="http://rexdouglass.com/extracting-data-from-printed-tables-in-historical-documents/">Dr. Rex Douglass’ (UCSD Polisci) method</a>,</strong> who use computer vision techniques to "cut up" tables, OCRing individual cells before reassembling the table. I recommend taking a peak at both to understand alternative workflows for table scraping. Other users have opted to detect tables after OCRing: first, recognizing text in PDF files and then stripping the OCRd content using <a href="tabula.nerdpower.org" target="_blank"><strong>PDF table extraction tools like Tabula</strong></a>. These methods hint to the growing hacker community interested in scraping PDF content. The recent <a href="http://pdfliberation.wordpress.com/"><strong>PDF Liberation Hackathon website</strong></a> features some great tools to this end.</p>
<p>Feel free to shoot me any feedback or share your experiences with digitizing historic data: <a href="mailto:nathaniel.lane@iies.su.se">nathaniel.lane AT iies.su.se</a>.</p>

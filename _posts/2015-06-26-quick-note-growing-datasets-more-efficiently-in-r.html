---
layout: post
title: Quick Note - Growing Datasets (More) Efficiently in R
date: 2015-06-26 11:57:06.000000000 +08:00
type: post
published: true
status: publish
categories:
- R
tags:
- Big Data Blah Blah Blah
- Code
- programming
meta:
  pe_theme_meta: O:8:"stdClass":2:{s:7:"gallery";O:8:"stdClass":3:{s:2:"id";s:2:"89";s:5:"width";s:0:"";s:6:"height";s:0:"";}s:5:"video";O:8:"stdClass":1:{s:2:"id";s:3:"504";}}
  _edit_last: '1'
author:
 "Nathaniel"
---
<p>
<div class="media image"><img src="{{ site.baseurl }}/assets/pushingworkers_fieldmuseum.jpg" alt="From the Field Museum collection" /></div>
<a href="http://fieldmuseumphotoarchives.tumblr.com/">From the Field Museum archives, 1920, Photographer Herbert P. Burtch, Oriental Institute. "Men moving Totem Pole outside Field Museum by train."</a>.<br />
&nbsp;</p>
<h2>The Usual Mumbo Jumbo.</h2>
<p><code>Note: This was originally some notes to RAs but I figured it may be useful for other people out there.</code><br />
I've had some discussion with econ folks and RAs who are working with giant datasets in R for the first time. In particular, those having to <em>"harvest"</em> or <em>"grow" </em>unweildy datasets. R is notoriously slow when it comes to expanding datasets, such as when you want to increntally append rows to a file with results from a scraping API, or combine a giant stack of raw text files from another text mining project.<br />
&nbsp;<br />
The usual "good" method for concatination uses a <code>do.call</code> function with the rbind function. This method essentially takes a list of stuff and passes them as arguments all at once to <code>rbind</code>. In other words, you can take a list of <em>data.frame</em> names and bind the rows together in one motion: <code>do.call("rbind", &lt;&lt;&lt;A list of data.frame names&gt;&gt;&gt;)</code>.<br />
&nbsp;<br />
A common task I encounter is grabbing a chunk of files from a directory and combining them into a dataset. Such a task requires three steps. First, generating a list of files from a directory that match a pattern (e.g. all the .csv files in a directory) using the <code>list.files</code> function. Next, looping over this list of files and loading them into R with with <code>lapply</code>, applying the <code>read.csv</code> function to a list of files. Then, finally, using <code>do.call</code> to <code>rbind</code>, or stack, all the loaded <em>.csv</em> files into a single dataset.<br />
&nbsp;</p>
<pre>
Something like this:
&nbsp;
# Grab the list of files in the directory "/home/user/foo" that end in ".csv"
csvlist&lt;-list.files(path = "/home/user/foo",
pattern = ".csv", all.files = FALSE,
full.names = TRUE, recursive = FALSE)
&nbsp;
# "Apply" the read.csv function to the list of csv files.
csvloaded&lt;-lapply(csvlist, read.csv)
&nbsp;
# Append the loaded .csv files into a list.
dataset1&lt;-do.call("rbind",csvloaded)
</pre>
<p>&nbsp;<br />
This is all great, but it can still take a ton of time. Below I condense the <code>lapply</code> function and the <code>do.call</code>+<code>rbind</code> line into one:<br />
&nbsp;</p>
<pre>
ptm &lt;- proc.time()
&nbsp;
dataset1&lt;-do.call("rbind",lapply(csvlist, read.csv))
&nbsp;
proc.time() - ptm
&nbsp;
&gt; user system elapsed
&gt; 48.840 0.148 50.241
</pre>
<p>&nbsp;<br />
If you're doing more complicated tasks or working with large sets of data, processing time can balloon.<br />
&nbsp;</p>
<h2>A faster method.</h2>
<p>Using <a href="https://github.com/Rdatatable/data.table/wiki">the <code>data.table</code> package</a> can speed things along if we're trying to get big data into R efficiently (<a href="I highly recommend checking out the github for the project">I highly recommend checking out the github for the project</a>).<br />
The <code>rbindlist</code> function included in the package is incredibly fast and written in C. In addition the <code>fread</code> function is built to efficiently read data into R.<br />
&nbsp;<br />
Below I replace the normal <code>read.csv</code> function with <code>fread</code>, and replace <code>do.call</code>+<code>rbind</code> with <code>rbindlist</code>.<br />
&nbsp;</p>
<pre>
library(data.table)
&nbsp;
dataset2&lt;-rbindlist(lapply(csvlist, fread))
&nbsp;
proc.time() - ptm
&nbsp;
&gt; user system elapsed
&gt; 4.044 0.084 4.144
</pre>
<p>&nbsp;<br />
Both methods deliver identical datasets but there are some real efficiency gains when using <code>fread</code> and <code>rbindlist</code> from the super useful <code>data.table</code> package.<br />
&nbsp;</p>
<pre>
identical(dataset1,dataset2)
&gt; TRUE
</pre>
<p>This can have pretty amazing payoffs when working trying to load massive data sets into R to process.</p>

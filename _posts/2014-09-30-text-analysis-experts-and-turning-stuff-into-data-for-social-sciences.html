---
layout: post
title: Text Analysis, (Non-) Experts, and Turning "Stuff" into Social Science Data
date: 2014-09-30 03:29:47.000000000 +08:00
type: post
published: true
status: publish
categories:
- Political Economy
tags:
- crowdsourcing
- data
- methods
- political science
meta:
  pe_theme_meta: O:8:"stdClass":2:{s:7:"gallery";O:8:"stdClass":3:{s:2:"id";s:2:"89";s:5:"width";s:0:"";s:6:"height";s:0:"";}s:5:"video";O:8:"stdClass":1:{s:2:"id";s:3:"262";}}
  _edit_last: '1'
author:
  login: landlaborcapital
  email: nathaniel.x.lane@gmail.com
  display_name: landlaborcapital
  first_name: ''
  last_name: ''
---
<p>Social scientists often use experts to "code" datasets; experts read some stuff and code that stuff using a coding rule (a right-left political spectrum, etc.). But this a slow, painful process for constructing a dataset</p>
<p>Below is an awesome, succinct Kickstarter seminar on how to categorize (political) text using crowd-sourcing from quant-y political scientist, Drew Conway (<a href="http://drewconway.com/" target="_blank">http://drewconway.com/</a>):</p>
<p><iframe src="//player.vimeo.com/video/80821921" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></p>
<p>The great lil' seminar is related to his recent working paper, "Methods for Collecting Large-scale Non-expert Text Coding":</p>
<h5>Abstract</h5>
<blockquote><p>The task of coding text for discrete categories or quantifiable scales is a classic problem in political science. Traditionally, this task is executed by qualified ``experts''.  While productive, this method is time consuming, resource intensive, and introduces bias.  In the following paper I present the findings from a series of experiments developed to assess the viability of using crowd-sourcing platforms for political text coding, and how variations in the collection mechanism affects the quality of output...
</p></blockquote>
